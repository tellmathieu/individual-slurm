{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbcfa1b4-4ca9-427d-a396-1b831b0d6800",
   "metadata": {},
   "source": [
    "## Setting Up a Simple Slurm Cluster on Amazon Web Services for Personal Use\n",
    "\n",
    "***Note: This does cost money, be sure to monitor costs.***\n",
    "\n",
    "#### Prerequisite:\n",
    "- AWS account (go here to make an account: https://aws.amazon.com/)\n",
    "\n",
    "#### Tutorial Used:\n",
    "- https://www.tecracer.com/blog/2023/05/hello-slurm-getting-started-with-high-performance-computing-on-aws.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87148b0e-2f50-466d-950e-bbc6942d0f8e",
   "metadata": {},
   "source": [
    "### Step 1: Launch an EC2 Instance\n",
    "1. Once logged into aws, type *ec2* into the search\n",
    "2. There should be a button on the page that says *Launch Instance*, click on that.\n",
    "3. File in information for the ec2 instance\n",
    "    - **Name:** enter a name that is relevant and memorable. Mine was *HPC-BASE13*.\n",
    "    - **Application and OS Image:** Amazon Linux 2023\n",
    "    - **Key Pair:** Use a pre-established key pair that you used before or create a new one. I imagine best practice is a new one. This will generate a *.pem* file. Save it to your computer, when you `ssh` in, this is like login credentials.\n",
    "    - **Network Settings:** I did not touch this.\n",
    "    - **Configure Storage:** I did not touch this.\n",
    "    - **Advanced Settings:** I only set the IAM Instance profile to an IAM profile that I gave admin access to. This is necessary or you will get errors.\n",
    "4. Click *Launch Instance*, and it'll get setup. The instance will be a blank slate with no files on it, so next we have to configure it to act as our slurm base.\n",
    "5. You can connect to the instance via the aws browser connection by clicking *Connect* and then on the next page using the *Connect using EC2 Instance Connect* option. I use the terminal command `ssh -i \"<ssh_key_name>.pem\" ec2-user@<unique_get_from_ssh_tab>.us-west-1.compute.amazonaws.com`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c04fe-dca2-4a62-bd5a-3c7cc92b8440",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Configure Slurm Base Instance\n",
    "\n",
    "The base instance you create here basically will do a lot of the cluster things automatically for you. Most of it is pulled directly from the tutorial used.\n",
    "\n",
    "1. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fda9e8-e734-45e2-83c6-f9ef5d46dc7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# switch user\n",
    "sudo su -\n",
    "su - ec2-user\n",
    "\n",
    "# install python3-pip and nodejs\n",
    "sudo yum install python3-pip nodejs -y\n",
    "\n",
    "# install and create a python virtual environment\n",
    "python3 -m pip install --user --upgrade virtualenv\n",
    "python3 -m virtualenv ~/hpc-ve\n",
    "\n",
    "# activate (or turn on) this virtual environment --- this will need to be done every time you login to the hpc-base, or you can set it up in the `~/.bashrc` file\n",
    "source ~/hpc-ve/bin/activate\n",
    "\n",
    "# install the aws cluster software that pretty much does everything for you. You might be able to get away with not setting the versions of these softwares, but I didn't try. \n",
    "pip install aws-parallelcluster==3.5.0 && pip install flask==2.2.3\n",
    "\n",
    "#check that you have it installed, it should be the version you specified\n",
    "pcluster version\n",
    "\n",
    "# use the software to help you configure your *.yaml* file that will help you set up for slurm base\n",
    "pcluster configure --config cluster-config.yaml --region=us-west-1\n",
    "\n",
    "#note: the yaml file name can be anything you want. It will be created with this command. If you want to edit it afterwards, you would use nano or vi\n",
    "#2nd note: you need to specify the region, and it needs to be the same as your ec2 instance (mine is called HPC-Base13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49217f8-1459-4c9a-b1b6-1395b3bd55e1",
   "metadata": {},
   "source": [
    "2. Choose slurm configuration (below is copied from the tutorial, because I don't know much about these settings and micro instances are on the aws free tier)\n",
    "    - Choose the EC2 Key Pair Name added in the prerequisites step\n",
    "    - Choose the scheduler to be used: slurm\n",
    "    - Choose the Operating System: alinux2 (Amazon Linux 2)\n",
    "    - Choose the HeadNode instance type: t3.micro\n",
    "    - Number of queues: default (1)\n",
    "    - Name of queue: default (queue1)\n",
    "    - Number of compute resources for queue1: default (1)\n",
    "    - Compute instance type for queue1: t3.micro\n",
    "    - Maximum instance count: default (10)\n",
    "    - Automate VPC creation: default\n",
    "    - Set VPC settings: no automated creation; choose your created subnets during the prerequisites step\n",
    "3. Alter the *.yaml* file, so that we can have a shared drive with extra space on it. I used part of the config file from the aws github for this software: https://github.com/aws/aws-parallelcluster/blob/release-3.0/cli/tests/pcluster/example_configs/awsbatch.simple.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7c06c-132c-4eca-b2f3-0063f4f302fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nano cluster-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f7a6d-130a-45ce-b035-a81e2977bfc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "  3. continued\n",
    "      - I typed the following information into the yaml file (with proper indenting to add the shared drive and saved:\n",
    "> SharedStorage:  \n",
    ">> MountDir: /sharet  \n",
    "Name: ebs2  \n",
    "StorageType: Ebs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09aeb94-e5f4-4716-b44f-f447e63f49bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my config file\n",
    "Region: us-west-1\n",
    "Image:\n",
    "  Os: alinux2\n",
    "HeadNode:\n",
    "  InstanceType: t3.micro\n",
    "  Networking:\n",
    "    SubnetId: <id_from_program>\n",
    "  Ssh:\n",
    "    KeyName: <ssh_key_name>\n",
    "Scheduling:\n",
    "  Scheduler: slurm\n",
    "  SlurmQueues:\n",
    "  - Name: queue1\n",
    "    ComputeResources:\n",
    "    - Name: t3micro\n",
    "      Instances:\n",
    "      - InstanceType: t3.micro\n",
    "      MinCount: 0\n",
    "      MaxCount: 10\n",
    "    Networking:\n",
    "      SubnetIds:\n",
    "      - <id_from_program>\n",
    "SharedStorage:\n",
    "  - MountDir: /sharet\n",
    "    Name: ebs2\n",
    "    StorageType: Ebs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3336e5f1-aacf-44f9-8b5e-4095ba3f707e",
   "metadata": {},
   "source": [
    "4. Run the command to create the cluster based on the config file. This is where you would specify the name of your cluster. It must be a new name you have used before. I used *cluster5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba1f5b-463c-4cf7-906e-52950c20422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcluster create-cluster --cluster-configuration cluster-config.yaml --cluster-name cluster5 --region us-west-1\n",
    "\n",
    "# to check the progress of the cluster creation\n",
    "pcluster describe-cluster --cluster-name cluster5 --region=us-west-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d9c7fd-3e61-48af-8a51-af3781a7cefe",
   "metadata": {},
   "source": [
    "### Step 3: Use Cluster\n",
    "\n",
    "You now want to `ssh` into the headnode (where you submit jobs), which is an instance that is auto-created by the hpc-base instance when you create the cluster using `pcluster`. You need to upload your *.pem* file into the cluster if it's not there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094cee9a-f402-4acc-887b-3838144ff691",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcluster ssh --cluster-name cluster5 -i ~/.ssh/<ssh_key_name>.pem --region=us-west-1\n",
    "\n",
    "#I altered the ~/.bashrc file so that the source command in step 2 and the above command are run automatically when I connect to the hpc-base instance, so that I go straight to the headnode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa2d2c-9ca4-4d8a-8fce-ac05dcfd6484",
   "metadata": {},
   "source": [
    "You can write a simple script (with the *.sh* file type, like `echo \"Hello World\"`) to do something and then test this by sending an sbatch command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e756d0d-d2cf-4225-ba2e-13da424527d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbatch script.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93390d-3f33-41ce-a62a-17a72bbbb94a",
   "metadata": {},
   "source": [
    "The cluster will create the compute node and process the job and spit out a *slurm-#.out* file with the results in the same working directory where you ran the `sbatch` command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1468608-97d2-44dc-bc22-b99ece7ec17f",
   "metadata": {},
   "source": [
    "### Step 4: Configuration and Actual Runs\n",
    "\n",
    "I added all my files to the /sharet file space (originally has the drive named \"share13\" but this gave me errors with the mamba environment, so that if I recreate the headnode (change the cluster config), then I have everything on the cluster's drive. I put my github files and my data files in different folders, just to make it easier.\n",
    "\n",
    "When running the sbatch code, I kept getting errors. It was an issue with RAM/mem, where my genomic mapping program, STAR, was not working. This was the error in the log:\n",
    "\n",
    "> EXITING: fatal error trying to allocate genome arrays, exception thrown: std::bad_alloc  \n",
    "Possible cause 1: not enough RAM. Check if you have enough RAM 2012033883 bytes  \n",
    "Possible cause 2: not enough virtual memory allowed with ulimit. SOLUTION: run ulimit -v 2012033883  \n",
    "  \n",
    "> Jan 10 03:55:20 ...... FATAL ERROR, exiting  \n",
    "\n",
    "It's taken a bit of troubleshooting. I think it's because I didn't set up up the config file correctly or I'm submitting an `sbatch` command that is not appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83eb010-2b48-4c2b-80f8-5a53c8e7c709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
